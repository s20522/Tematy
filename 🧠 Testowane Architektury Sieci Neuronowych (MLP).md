#  Testowane Architektury Sieci Neuronowych (MLP)

_Data: 10 lutego 2026_  
_Autor: Manus AI_  
_Wersja: 1.0_

---

## 1. Wprowadzenie

Niniejszy dokument szczeg贸owo opisuje **7 architektur wielowarstwowego perceptronu (MLP)**, kt贸re zostay przetestowane w ramach pracy in偶ynierskiej w celu predykcji miertelnoci pacjent贸w z niewydolnoci serca. Celem eksperyment贸w byo zbadanie wpywu gbokoci i szerokoci sieci na jej zdolno do generalizacji i skuteczno predykcyjn na maym, tabelarycznym zbiorze danych.

### 1.1. Kontekst Eksperymentu

- **Zbi贸r danych:** 299 pacjent贸w, 3 cechy wejciowe (`age`, `ejection_fraction`, `serum_creatinine`).
- **Problem:** Klasyfikacja binarna (prze偶y / zmar).
- **Cel:** Znalezienie optymalnej architektury MLP, kt贸ra zr贸wnowa偶y zo偶ono modelu z ryzykiem przeuczenia.

### 1.2. Wsp贸lne Parametry Treningu

Wszystkie testowane architektury byy trenowane z u偶yciem nastpujcych, staych hiperparametr贸w, aby zapewni por贸wnywalno wynik贸w:

| Parametr | Warto | Uzasadnienie |
|---|---|---|
| **Funkcja aktywacji (warstwy ukryte)** | `ReLU` | Standardowy wyb贸r, zapobiega zanikajcym gradientom. |
| **Funkcja aktywacji (warstwa wyjciowa)** | `Sigmoid` | Zwraca prawdopodobiestwo przynale偶noci do klasy 1 (mier). |
| **Optymalizator** | `Adam` | Skuteczny i szybki, standard w wikszoci problem贸w. |
| **Funkcja straty** | `BinaryCrossentropy` | Standardowa funkcja straty dla klasyfikacji binarnej. |
| **Liczba epok** | `100` | Wystarczajca do osignicia zbie偶noci. |
| **Batch size** | `32` | Dobry kompromis midzy szybkoci a stabilnoci treningu. |
| **Metryka do oceny** | `AUC` | Dobrze radzi sobie z niezbalansowanymi danymi. |

---

## 2. Opis Testowanych Architektur

Architektury zostay podzielone na trzy kategorie: **pytkie**, **rednie** i **gbokie**, aby systematycznie bada wpyw zo偶onoci modelu.

### 2.1. Architektury Pytkie (1 warstwa ukryta)

Charakteryzuj si jedn warstw ukryt, co ogranicza ich zo偶ono i ryzyko przeuczenia. Testowano trzy warianty r贸偶nice si liczb neuron贸w.

#### **Architektura 1: `Shallow_32`**
- **Struktura:** `[Input(3)] -> [Dense(32, ReLU)] -> [Dense(1, Sigmoid)]`
- **Liczba parametr贸w:** ~161
- **Uzasadnienie:** Minimalistyczny model, sprawdzajcy, czy bardzo prosta sie jest w stanie nauczy si zale偶noci w danych.

#### **Architektura 2: `Shallow_64`**
- **Struktura:** `[Input(3)] -> [Dense(64, ReLU)] -> [Dense(1, Sigmoid)]`
- **Liczba parametr贸w:** ~257
- **Uzasadnienie:** Zwikszenie liczby neuron贸w w celu uchwycenia bardziej zo偶onych interakcji.

#### **Architektura 3: `Shallow_128`** 
- **Struktura:** `[Input(3)] -> [Dense(128, ReLU)] -> [Dense(1, Sigmoid)]`
- **Liczba parametr贸w:** ~513
- **Uzasadnienie:** Dalsze zwikszenie pojemnoci modelu. **Okazaa si najlepsz architektur w tej kategorii.**

### 2.2. Architektury rednie (2 warstwy ukryte)

Dodanie drugiej warstwy ukrytej pozwala modelowi na nauk bardziej abstrakcyjnych i hierarchicznych reprezentacji danych. Testowano dwie popularne konfiguracje w ksztacie "lejka".

#### **Architektura 4: `Medium_64_32`**
- **Struktura:** `[Input(3)] -> [Dense(64, ReLU)] -> [Dense(32, ReLU)] -> [Dense(1, Sigmoid)]`
- **Liczba parametr贸w:** ~2401
- **Uzasadnienie:** Klasyczna architektura "lejka", gdzie ka偶da kolejna warstwa ma mniej neuron贸w, co zmusza sie do kompresji informacji.

#### **Architektura 5: `Medium_128_64`**
- **Struktura:** `[Input(3)] -> [Dense(128, ReLU)] -> [Dense(64, ReLU)] -> [Dense(1, Sigmoid)]`
- **Liczba parametr贸w:** ~8833
- **Uzasadnienie:** Bardziej pojemna wersja architektury "lejka", zdolna do modelowania bardziej skomplikowanych zale偶noci.

### 2.3. Architektury Gbokie (3 warstwy ukryte)

Trzy warstwy ukryte reprezentuj podejcie "deep learning" i maj najwikszy potencja do przeuczenia na maym zbiorze danych. Testowano dwie konfiguracje.

#### **Architektura 6: `Deep_128_64_32`**
- **Struktura:** `[Input(3)] -> [Dense(128, ReLU)] -> [Dense(64, ReLU)] -> [Dense(32, ReLU)] -> [Dense(1, Sigmoid)]`
- **Liczba parametr贸w:** ~10,977
- **Uzasadnienie:** Gboka sie o du偶ej pojemnoci, testujca granice zo偶onoci dla tego problemu.

#### **Architektura 7: `Deep_256_128_64`**
- **Struktura:** `[Input(3)] -> [Dense(256, ReLU)] -> [Dense(128, ReLU)] -> [Dense(64, ReLU)] -> [Dense(1, Sigmoid)]`
- **Liczba parametr贸w:** ~42,625
- **Uzasadnienie:** Bardzo gboka i szeroka sie, majca na celu sprawdzenie, czy ekstremalna zo偶ono przyniesie korzyci.

---

## 3. Wyniki i Analiza

### 3.1. Tabela Por贸wnawcza Wynik贸w

Poni偶sza tabela przedstawia wyniki metryk F1-score, Recall, Precision i AUC dla ka偶dej z testowanych architektur na zbiorze testowym.

| Kategoria | Architektura | F1-score | Recall | Precision | AUC | Liczba Parametr贸w |
|---|---|---|---|---|---|---|
| **Pytkie** | `Shallow_32` | 0.541 | 0.632 | 0.478 | 0.748 | 161 |
| | `Shallow_64` | 0.564 | 0.579 | 0.550 | 0.758 | 257 |
| | **`Shallow_128`**  | **0.591** | **0.684** | **0.520** | **0.763** | **513** |
| **rednie** | `Medium_64_32` | 0.558 | 0.632 | 0.500 | 0.758 | 2,401 |
| | `Medium_128_64` | 0.579 | 0.579 | 0.579 | 0.763 | 8,833 |
| **Gbokie** | `Deep_128_64_32` | 0.550 | 0.579 | 0.524 | 0.753 | 10,977 |
| | `Deep_256_128_64` | 0.564 | 0.684 | 0.481 | 0.753 | 42,625 |

### 3.2. Kluczowe Obserwacje

1.  **Prostota wygrywa:** Najlepsze wyniki osigna najprostsza kategoria architektur (pytkie). Model `Shallow_128` uzyska najwy偶szy F1-score (0.591) i Recall (0.684).

2.  **Gbsze nie znaczy lepsze:** Zwikszanie gbokoci (2 i 3 warstwy) nie przynioso poprawy wynik贸w, a wrcz je pogorszyo. Modele gbokie miay tendencj do przeuczania, co wida po ni偶szych wartociach metryk na zbiorze testowym.

3.  **Liczba parametr贸w a wydajno:** Nie zaobserwowano prostej korelacji midzy liczb parametr贸w a skutecznoci. Najlepszy model mia tylko 513 parametr贸w, podczas gdy najgorszy z modeli gbokich mia ich ponad 42,000.

4.  **Recall vs Precision:** Architektury `Shallow_128` i `Deep_256_128_64` osigny najwy偶szy Recall (0.684), co jest kluczowe w zastosowaniach medycznych. Jednak model `Shallow_128` mia znacznie lepsz precyzj, co czyni go bardziej zr贸wnowa偶onym.

---

## 4. Wnioski

Dla analizowanego problemu i zbioru danych, **prosta, pytka architektura sieci neuronowej z jedn warstw ukryt i 128 neuronami (`Shallow_128`) okazaa si najskuteczniejsza**. Bardziej zo偶one, gbsze modele nie byy w stanie wykorzysta swojej pojemnoci i wykazyway tendencj do przeuczania, co jest typowym zjawiskiem na maych zbiorach danych.

Wyniki te sugeruj, 偶e w przypadku problem贸w z ograniczon iloci danych, nale偶y preferowa prostsze modele, kt贸re s mniej podatne na przeuczenie i atwiejsze w interpretacji. Zwikszanie zo偶onoci architektury bez odpowiedniej iloci danych nie gwarantuje lepszych wynik贸w.
